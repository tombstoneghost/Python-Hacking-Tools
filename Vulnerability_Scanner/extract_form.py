# Imports
import requests
import optparse
import urllib.parse as urlparse
from bs4 import BeautifulSoup


# Getting arguments from CLI
def get_arguments():
    parser = optparse.OptionParser()
    parser.add_option("-t", "--target", dest="target", help="Specify Target URL to scan")

    (options, arguments) = parser.parse_args()

    if not options.target:
        parser.error("[-] Please specify a target URL, use --help for more info.")

    return options


# Get Request Data
def request(url):
    try:
        return requests.get("http://" + url)
    except requests.exceptions.ConnectionError:
        pass


# Target URL
target_url = get_arguments().target
response = request(target_url)

parsed_html = BeautifulSoup(response.content)

# Extracting Forms
forms_list = parsed_html.findAll('form')

for form in forms_list:
    action = form.get("action")
    post_url = urlparse.urljoin(target_url, action)
    method = form.get("method")

    input_list = form.findAll("input")
    post_data = {}

    for input_tag in input_list:
        input_name = input_tag.get("name")
        input_type = input_tag.get("type")
        input_value = input_tag.get("value")

        if input_type == 'text':
            input_value = "test"

        post_data[input_name] = input_value

    result = requests.post(post_data, data=post_data)
    print(result.content)
